{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "369378431af54073ad74d3259648d67e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ffdb03d27cb44ab1976b704aa5dcc9c3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_565c525fcbfd4114a896bc8d40983bf8",
              "IPY_MODEL_ce0a824e8dde472bb869c569a5b58a81"
            ]
          }
        },
        "ffdb03d27cb44ab1976b704aa5dcc9c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "565c525fcbfd4114a896bc8d40983bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_555fe91350b04d5e91b533c10e025e83",
            "_dom_classes": [],
            "description": "Training: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 10,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 10,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fbf7c3562fb04db3adb1ab1a510755f2"
          }
        },
        "ce0a824e8dde472bb869c569a5b58a81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b6583579f4ea488694744bb9958a00b5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 10/10 [58:51&lt;00:00, 353.18s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_09202ce93a8b416c8220859fef3e4f74"
          }
        },
        "555fe91350b04d5e91b533c10e025e83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fbf7c3562fb04db3adb1ab1a510755f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b6583579f4ea488694744bb9958a00b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "09202ce93a8b416c8220859fef3e4f74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikithaBangera/COMP6721_Artificial_Intelligence/blob/main/Part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uqKjOUWVqvE"
      },
      "source": [
        "#Mounting the drive containing the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hLkDNyfH5fO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53d86bb7-abfe-44bf-f49d-33177870d713"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxvSISKL-dJH"
      },
      "source": [
        "# Importing the required libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIFcfiPrO4Eu"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.datasets import ImageFolder, DatasetFolder\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "from torchvision.transforms import ToTensor\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUDfjKitVQHT"
      },
      "source": [
        "# Training Dataset Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPbXkFBB-_N9"
      },
      "source": [
        "Loading the dataset from the directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7q5Eo1opSUe"
      },
      "source": [
        "def load_dataset(directory_path):\n",
        "  image_transforms = transforms.Compose(\n",
        "                    [transforms.Resize((32,32)),\n",
        "                     transforms.ToTensor(),\n",
        "                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                    ])\n",
        "  \n",
        "  dataset = ImageFolder(\n",
        "                        root = directory_path,\n",
        "                        transform = image_transforms\n",
        "                        )\n",
        "  return dataset\n",
        "\n",
        "directory_path = \"/content/drive/My Drive/AI Dataset/data/\"\n",
        "dataset = load_dataset(directory_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ItPmuUDWhY-"
      },
      "source": [
        "Dataset classes/labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8FwSxPJSTxD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b631d869-68ac-435d-fa6e-03e136c953d9"
      },
      "source": [
        "classes = dataset.classes\n",
        "classes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['not_a_person', 'with_mask', 'without_mask']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoueQpsaXCbK"
      },
      "source": [
        "Counting the number of images present in each of the classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x6i6QvURBvh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "b1462d10-4904-4c8f-c6f3-d56712278b82"
      },
      "source": [
        " def data_class_distribution(dataset):\n",
        "  data_class_count = {}\n",
        "  for _, index in dataset:\n",
        "    label = classes[index]\n",
        "    if label not in data_class_count:\n",
        "      data_class_count[label] = 0\n",
        "    data_class_count[label] += 1\n",
        "  return data_class_count\n",
        "    \n",
        " data_class_count = data_class_distribution(dataset)\n",
        " print(\"The number of images present in each of the three classes:\", data_class_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-60e1835b385e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mimage_class_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mimage_class_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_class_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The number of images present in each of three classes: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_class_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-60e1835b385e>\u001b[0m in \u001b[0;36mget_class_count\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_class_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mimage_class_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_class_count\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[1;32m    150\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2816\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2818\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2820\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmjXUs6v_f26"
      },
      "source": [
        "# Dataset Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_JApP6Yg69u"
      },
      "source": [
        "def plot_dataset_images(file_path, images_list):\n",
        "  rows=20 #rows in subplots\n",
        "  cols=5 #columns in subplots\n",
        "\n",
        "  fig,ax = plt.subplots(rows,cols,figsize=(12,100))\n",
        "  r = 0\n",
        "  c = 0\n",
        "  for i in range(rows*cols):\n",
        "    aa = plt.imread(os.path.join(file_path, images_list[i]))\n",
        "    ax[r,c].axis(\"off\")\n",
        "    ax[r,c].imshow(aa)\n",
        "      c+=1\n",
        "      if c == cols:\n",
        "        c=0\n",
        "        r+=1\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP3jqqXVfLPo"
      },
      "source": [
        "1. Sample images of the class \"With a mask\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaTzfnrcfBuC"
      },
      "source": [
        "with_mask_file_path = \"/content/drive/My Drive/AI Dataset/data/with_mask\"\n",
        "with_mask_images = os.listdir(\"/content/drive/My Drive/AI Dataset/data/with_mask\")\n",
        "\n",
        "plot_dataset_images(with_mask_file_path, with_mask_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZFFrFTiexl_"
      },
      "source": [
        "2. Sample images of the class \"Without a mask\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "locc5HGseyQx"
      },
      "source": [
        "without_mask_file_path = \"/content/drive/My Drive/AI Dataset/data/without_mask\"\n",
        "without_mask_images = os.listdir(\"/content/drive/My Drive/AI Dataset/data/without_mask\")\n",
        "\n",
        "plot_dataset_images(without_mask_file_path, without_mask_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWhtJ_4vfapf"
      },
      "source": [
        "3. Sample images of the class \"Not a person\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlgqP3l0fqd-"
      },
      "source": [
        "not_a_person_file_path = \"/content/drive/My Drive/AI Dataset/data/not_a_person\"\n",
        "not_a_person_images = os.listdir(\"/content/drive/My Drive/AI Dataset/data/not_a_person\")\n",
        "\n",
        "plot_dataset_images(not_a_person_file_path, not_a_person_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNrHsN-BebjZ"
      },
      "source": [
        "Categorical representation of the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCAdFexets04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "outputId": "d21939a6-4a5d-47da-ba19-21d6f699d158"
      },
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "sns.barplot(data = pd.DataFrame.from_dict([get_class_distribution(dataset)]).melt(), x = \"variable\", y=\"value\", hue=\"variable\").set_title('Face Mask Class Distribution')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YDkIs9s_pt2"
      },
      "source": [
        "Splitting the Data into train, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UWi89bbt2iB"
      },
      "source": [
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = (len(dataset) - train_size) * 0.5\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, (train_size, test_size, test_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oAJAz0u_yF3"
      },
      "source": [
        "# Loading Data with the help of DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SHjWmQNt7Ge",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef10922f-8795-4735-a84a-4ccc4081ef0d"
      },
      "source": [
        "train_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=4, num_workers=2)\n",
        "val_loader = DataLoader(dataset=val_dataset, shuffle=False, batch_size=4, num_workers=2)\n",
        "test_loader = DataLoader(dataset=test_dataset, shuffle=False, batch_size=4, num_workers=2)\n",
        "print(\"Length of the train_loader:\", len(train_loader))\n",
        "print(\"Length of the val_loader:\", len(val_loader))\n",
        "print(\"Length of the val_loader:\", len(test_loader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of the train_loader: 3601\n",
            "Length of the val_loader: 901\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XeDSNTVCHrm"
      },
      "source": [
        "# Convolutional Neural Network Architecture and model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62aUXwwQdDU1"
      },
      "source": [
        "CNN Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "721oayHAdBDC"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    self.conv_layer = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.LeakyReLU(inplace=True),\n",
        "        nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.LeakyReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.LeakyReLU(inplace=True),\n",
        "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.LeakyReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.LeakyReLU(inplace=True),\n",
        "        nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.LeakyReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "\n",
        "    self.fully_connected_layer = nn.Sequential(\n",
        "        nn.Dropout(p=0.1),\n",
        "        nn.Linear(8 * 8 * 32, 1000),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(1000, 512),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout(p=0.1),\n",
        "        nn.Linear(512, 128),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout(p=0.1),\n",
        "        nn.Linear(128, 3)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    #convolutional layers\n",
        "    x = self.conv_layer(x)\n",
        "\n",
        "    #flattening the layers\n",
        "    x = x.view(x.size(0), -1)\n",
        "\n",
        "    #fully connected layer\n",
        "    x = self.fully_connected_layer(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLRl7WRQCNT0"
      },
      "source": [
        "CNN model, loss function and Optimizer initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkT2i-etuHp5"
      },
      "source": [
        "net = CNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#using Stochastic Gradient Descent\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnl6xA4-CTwT"
      },
      "source": [
        "Training the CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c31qM2vGuOCL"
      },
      "source": [
        "def images_to_probs(net, images):\n",
        "    output = net(images)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, preds_tensor = torch.max(output, 1)\n",
        "    preds = np.squeeze(preds_tensor.numpy())\n",
        "    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
        "\n",
        "def evaluate(net, dataloader):\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        net.eval()\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images, labels.numpy()\n",
        "\n",
        "            preds, probs = images_to_probs(net, images)\n",
        "\n",
        "            total += len(labels)\n",
        "            correct += (preds == labels).sum()\n",
        "    return correct/total * 100\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def train(net, loss_fn, opt, dataloader, epochs):\n",
        "    num_steps = 0\n",
        "    min_loss = 1e+10\n",
        "\n",
        "    for epoch in tqdm(range(1, epochs+1), total=epochs, desc='Training'):\n",
        "        running_loss = []\n",
        "        net.train() # Setting the network to TRAIN mode\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images, labels\n",
        "            num_steps += 1\n",
        "\n",
        "            # FP\n",
        "            outs = net(images)\n",
        "            loss = loss_fn(outs, labels)\n",
        "\n",
        "            # Logging the loss value\n",
        "            running_loss.append(loss.item())\n",
        "\n",
        "            # BP\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            # Clearing the RAM\n",
        "            #del images, labels, outs\n",
        "            #torch.device('cpu').empty_cache()\n",
        "        \n",
        "        epoch_loss = sum(running_loss) / len(running_loss)\n",
        "        acc = evaluate(net, dataloader)\n",
        "        print(\"Accuracy: \",acc)\n",
        "        print(\"loss: \",epoch_loss)\n",
        "        # Model Checkpointing\n",
        "        if epoch_loss < min_loss:\n",
        "            min_loss = epoch_loss\n",
        "            bestmodel = net.state_dict()\n",
        "    #torch.save(bestmodel,'{0}_{1:0.4f}.pth'.format('classifier',min_loss))\n",
        "    return None\n",
        "\n",
        "    print('Training finished!!!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibbaSGkXuPTv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "369378431af54073ad74d3259648d67e",
            "ffdb03d27cb44ab1976b704aa5dcc9c3",
            "565c525fcbfd4114a896bc8d40983bf8",
            "ce0a824e8dde472bb869c569a5b58a81",
            "555fe91350b04d5e91b533c10e025e83",
            "fbf7c3562fb04db3adb1ab1a510755f2",
            "b6583579f4ea488694744bb9958a00b5",
            "09202ce93a8b416c8220859fef3e4f74"
          ]
        },
        "outputId": "4ec03f49-69aa-4d88-8915-68ef05b8356d"
      },
      "source": [
        "num_epochs = 10\n",
        "train(net, criterion, optimizer, train_loader, num_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "369378431af54073ad74d3259648d67e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Training', max=10.0, style=ProgressStyle(description_widtâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  90.56512079977784\n",
            "loss:  0.4546296715434957\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  95.2374340460983\n",
            "loss:  0.2330034772198422\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  96.58428214384894\n",
            "loss:  0.16041571948974578\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  96.67453485143017\n",
            "loss:  0.11400051359874333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  98.81282976950847\n",
            "loss:  0.07863925507610567\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  98.60455429047487\n",
            "loss:  0.06158120785588171\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  99.18772563176896\n",
            "loss:  0.050516159285522784\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  98.88225492918635\n",
            "loss:  0.04061173087584998\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  99.48625381838379\n",
            "loss:  0.03070037082696034\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  99.65287420161066\n",
            "loss:  0.025951003238280534\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JosOlrw4Ca4c"
      },
      "source": [
        "# Testing Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f4Uz0SUuRm4"
      },
      "source": [
        "testimg, testlab = next(iter(val_loader))\n",
        "testimg, testlab = testimg, testlab\n",
        "\n",
        "pred = net(testimg)\n",
        "predlab = torch.argmax(pred,axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7F054tXChuf"
      },
      "source": [
        "# Prediction \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRsSgjxguUz4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cb8b969-68c1-4ed2-b977-e5e522cb30bb"
      },
      "source": [
        "print(\"Predicted\",[i for i in predlab])\n",
        "print(\"Actual\",[i for i in testlab])\n",
        "for i in predlab:\n",
        "  if i == 0:\n",
        "    print(\"without_mask\")\n",
        "  elif i ==1:\n",
        "    print(\"with_mask\")\n",
        "  else :\n",
        "    print(\"not_a_person\")\n",
        "print(\"______________________________________________________________________________________________________________\")\n",
        "for i in testlab:\n",
        "  if i == 0:\n",
        "    print(\"without_mask\")\n",
        "  elif i ==1:\n",
        "    print(\"with_mask\")\n",
        "  else :\n",
        "    print(\"not_a_person\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted [tensor(0), tensor(2), tensor(1), tensor(0)]\n",
            "Actual [tensor(0), tensor(2), tensor(1), tensor(0)]\n",
            "without_mask\n",
            "not_a_person\n",
            "with_mask\n",
            "without_mask\n",
            "______________________________________________________________________________________________________________\n",
            "without_mask\n",
            "not_a_person\n",
            "with_mask\n",
            "without_mask\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTbun3w6Cl6I"
      },
      "source": [
        "# Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdQUEphVuZri",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e7c158b-9042-479c-beaf-5604f10b9f14"
      },
      "source": [
        "predlab = predlab.to(torch.device('cpu'))\n",
        "testlab = testlab.to(torch.device('cpu'))\n",
        "pred = predlab.numpy()\n",
        "test = testlab.numpy()\n",
        "matrix = confusion_matrix(test,pred)\n",
        "print(matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2 0 0]\n",
            " [0 1 0]\n",
            " [0 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgU3RnHVCsDI"
      },
      "source": [
        "# Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7fn-Y5hucwf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ee497f3-612f-4ce9-ac9b-e078f4c62f52"
      },
      "source": [
        "report = classification_report(test, pred, target_names=['with_mask','without_mask','not_a_person'])\n",
        "print(report)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   with_mask       1.00      1.00      1.00         2\n",
            "without_mask       1.00      1.00      1.00         1\n",
            "not_a_person       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           1.00         4\n",
            "   macro avg       1.00      1.00      1.00         4\n",
            "weighted avg       1.00      1.00      1.00         4\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W54n5NJHFBzS"
      },
      "source": [
        "# Convolutional Neural Network Architecture and model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6G_nY-jO2BZ"
      },
      "source": [
        "#requires modifications as per the image pixel and the number of labels\n",
        "class CNN(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    self.conv_layer = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.LeakyReLU(inplace=True),\n",
        "        nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.LeakyReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.LeakyReLU(inplace=True),\n",
        "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.LeakyReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.LeakyReLU(inplace=True),\n",
        "        nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.LeakyReLU(inplace=True),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "\n",
        "    self.fc_layer = nn.Sequential(\n",
        "        nn.Dropout(p=0.1),\n",
        "        nn.Linear(8 * 8 * 32, 1000),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(1000, 512),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout(p=0.1),\n",
        "        nn.Linear(512, 128),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout(p=0.1),\n",
        "        nn.Linear(128, 3)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    #convoltional layers\n",
        "    x = self.conv_layer(x)\n",
        "\n",
        "    #flatten the layers\n",
        "    x = x.view(x.size(0), -1)\n",
        "\n",
        "    #fc layer\n",
        "    x = self.fc_layer(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXvqoLZrWtFU"
      },
      "source": [
        "Hyper-paramaters definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFhiVbVkWqxu"
      },
      "source": [
        "num_epochs = 10\n",
        "num_classes = 3\n",
        "learning_rate = 0.001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhXK9Oe-VlIJ"
      },
      "source": [
        "# Convolutional Neural Network Model Evaluation"
      ]
    }
  ]
}